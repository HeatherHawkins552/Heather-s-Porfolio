---
title: "MTG_part2"
author: "Heather Hawkins"
date: "2023-03-30"
output: html_document
---
```{r load-packages, message = FALSE}
library(rvest)
library(skimr)
library(glue)
library(tidyverse) 
library(usethis)
library(robotstxt)
```

Okay, last one failed, so lets try something different, lets view all of the MTG set lists and scrape them from something that is not tcg player.
---------------------------------------

``{r redo}
urls <- "https://scryfall.com/sets"

#now make a function for this

scrape_page <- function(url) {

  #Then learn to read the url
  
page <- read_html(url)

### This time.. want to know the set title, number of cards in the set, and the date it was released.

titles <- page %>%
  html_nodes(".flexbox a")  %>%
  html_node("") %>%
  html_text() %>% 
  str_squish()

number <- page %>%
  html_nodes(".flexbox+ td a") %>%
  html_text()

date <- page %>%
  html_nodes(".flexbox~ td+ td a") %>%
  html_text()

  tibble(set_title = titles, 
         card_amount = number,
         date_released = date)
  
}

root <- "https://scryfall.com/sets"
numbers1 <- seq(from = 1, to = 1, by = 1)
urls1 <- glue("{root}{numbers1}")

#and now make a dataframe (ill cry if it doesnt work)
mtg_sets.df <- map_dfr(urls, scrape_page)

write_csv(mtg_sets.df, file = "mtg_sets.csv")

mtg_sets <- read_csv("mtg_sets.csv")
``

-----------------------------------------------

I'm crying. Lets try something else. 

Let's go big. Every card. 

But first lets start with the first page...

```{r another}
url_one <- "https://deckbox.org/games/mtg/cards?p=1"

#now make a function for this

scrape_page <- function(url) {

  #Then learn to read the url
  
page <- read_html(url)

# This time.. want to know the set title, number of cards in the srt, and the date it was released.

titles <- page %>%
  html_nodes(".card_name")  %>%
  html_node("") %>%
  html_text() %>% 
  str_squish()

price <- page %>%
  html_nodes("..center+ .minimum_width") %>%
  html_text()

type <- page %>%
  html_nodes("td:nth-child(4)") %>%
  html_text()

costandcolor <- page %>%
  html_nodes(".minimum_width") %>%
  html_text()

  tibble(card_title = titles, 
         card_price = price,
         card_type = type,
         card_cost_and_color = costandcolor)
  
}


root <- "https://deckbox.org/games/mtg/cards?p="
numbers1 <- seq(from = 1, to = 1, by = 25)
urls1 <- glue("{root}{numbers1}")


```
Works.. for now. 
----------------------
okay this didnt work 
--------

 (map_dfr(urls1,scrape_page)
write_rds(mtg_cards.df, "mtg_cards.RDS")
mtg_cards<-map_dfr(urls1,scrape_page)
-------

So im going to use an xpath to scrape and create a dataset

```{r okay}


xpath1 <- '//*[@id="show_simple_contents"]/div/table'

mtg_cards <- urls1 %>%                
  read_html() %>%
  html_nodes(xpath=xpath1) %>%
  html_table()  
mtg_cards <- mtg_cards[[1]]
mtg_cards <- mtg_cards[-1,] # Removing first row, which contains table headers


mtg_cards <- mtg_cards[,c(1,2,3,4)]          # Select column for Name, Price, Min Price, and Type
names(mtg_cards) <- c("Name", "N/A", "Min Price", "Type")       # Adding column names (overall price got deleted somehow :())

write_csv(mtg_cards, file = "mtg_cards_first10.csv")

```

I got the first ten items!!! Now I can see this as a clear list. Lets do EVERYTHING next time!! :)
